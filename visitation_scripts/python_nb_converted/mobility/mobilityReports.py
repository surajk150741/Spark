print("starting...")



# -*- coding: utf-8 -*-
"""part_1_mobility_script_generic(new_dataset)_new (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGNHBaJAS1w2hsZw4c_Kx4bFdQ0Kzdg_
"""

print("starting...")

from pyspark.sql.window import Window 
import pyspark
from pyspark import SparkContext

from pyspark.sql import functions as F
import pyspark.sql.functions as func


from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.sql.functions import col, window
import pyspark.sql.functions as func
from pyspark.sql.functions import desc
import datetime, time
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.window import Window
import pyspark.sql.functions as sf

import datetime as dt
import numpy as np
import os

from multiprocessing.pool import Pool
import pickle
import pandas as pd

from h3 import h3


from math import radians, cos, sin, asin, sqrt
from pyspark.sql.functions import lit
import json

from datetime import datetime, date, timedelta

#spark.conf.set("fs.azure.account.key.iasocialdata.dfs.core.windows.net",
#"5NXk2QYSRf4tbGSG1Sx20Seg6lkABbERa7ao2h0ZxWbSkLQtuy3+9yYDgr6+FDRKy1Rloi0d2dWa2XmF657SwQ==")
#spark.conf.set("spark.sql.execution.arrow.enabled", "true")  #### Enable Arrow-based columnar data transfers

#sc.install_pypi_package("boto3==1.16.35")
#sc.install_pypi_package("SqlAlchemy")
#sc.install_pypi_package("pymysql")

# from pyspark.sql import SparkSession
# spark = SparkSession.builder.appName("S3CSVRead").getOrCreate()

from pyspark.sql import SparkSession
import sys
import boto3
from io import BytesIO

from functools import reduce
from pyspark.sql import DataFrame

from pyspark.sql.functions import round, col
from dateutil import tz
import sqlalchemy as db


## oci imports
import oci
import os
import io
import sys
from pathlib import Path
from oci.config import validate_config
from oci.object_storage import ObjectStorageClient

#ociconf = oci.config.from_file()

spark = SparkSession.builder.appName("mobility_Reports") \
        .config("spark.default.parallelism", '120')\
        .config("spark.scheduler.mode", "FAIR") \
        .config("spark.pyspark.virtualenv.enabled", "true") \
        .config("spark.delta.logStore.oci.impl","io.delta.storage.OracleCloudLogStore")\
        .config("fs.oci.client.custom.authenticator", "com.oracle.bmc.hdfs.auth.InstancePrincipalsCustomAuthenticator")\
        .config('fs.oci.client.hostname', "https://objectstorage.us-ashburn-1.oraclecloud.com")\
        .config('fs.oci.client.auth.tenantId.region', "us-ashburn-1")\
        .getOrCreate()
#        .config("spark.driver.memory", "56g") \
#        .config("spark.driver.cores", "4") \
#        .config("spark.executor.memory", "20g") \
#        .config("spark.executor.cores", "2") \
#        .config("spark.executor.instances", '61')\
#        .config("spark.dynamicAllocation.enabled", "true") \
#        .config("spark.shuffle.service.enabled", "true")\
#        .config("spark.network.timeout","300")\
        
#        .config("spark.cores.max", "190") \
#spark.conf.set('fs.oci.client.auth.tenantId', "ocid1.tenancy.oc1..aaaaaaaamqxmfclvmrazpk4kt7ibkcfzfg2fvg4o2wyzemzu3n7tcpf6nvsa")
#spark.conf.set('fs.oci.client.auth.userId', "ocid1.user.oc1..aaaaaaaa4gjumyz4kiowhzbyjzgajyo5bln565rntgboqod3t6lu4bpwl2ra")
#spark.conf.set('fs.oci.client.auth.fingerprint', "ec:8f:0e:14:e0:31:cd:e4:d6:03:04:aa:aa:94:f9:a9")
#spark.conf.set('fs.oci.client.auth.pemfilepath', "~/.oci/oci_api_key")
#spark.conf.set('fs.oci.client.auth.tenantId.region', "us-ashburn-1")
#spark.conf.set('fs.oci.client.hostname', "https://objectstorage.us-ashburn-1.oraclecloud.com")
print("sparkContext")
print(spark.sparkContext._conf.getAll())
#print(ociconf)
print(spark.sparkContext.defaultParallelism)



##Arguments##

input_date = sys.argv[1] #"2021-05-02"
dataset_name=sys.argv[2] #"LS"
unique_token = sys.argv[3] #"06072021"
baseline_type = sys.argv[4] #"Mar21"
normalization_type =sys.argv[5] #"Mar21"
country = sys.argv[6]
days=20

# polygon_data_path = "s3://ia-customer-insights/required_files/state_city_h3_mapping.csv.gz"

##Arguments##


# --------------------------------------------------------------------------------
days = (datetime.now() - dt.datetime.strptime(input_date, "%Y-%m-%d")).days
tminus = date.today() - timedelta(days=int(days))
print(tminus)
date_string = tminus.strftime('%B%d')
print(date_string)

# token_date = date_string.strftime('%B_%d')
#token = requestId = "{}_{}".format(input_date, unique_token)
token = requestId = unique_token

print(token, date_string)
# --------------------------------------------------------------------------------
namespace = "bmmp5bv7olp2"

signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
identity_client = oci.identity.IdentityClient(config={}, signer=signer)

def createLogs():
    try:
        f = BytesIO()
    
        more_binary_data = str(f.getvalue().decode()) + "request_id="+requestId+";notebook_name=Mobility Reports;start_time="+ str(time.time())+ ";message=Started Mobility Changes generation\n"
        #analytics_logs = str(f.getvalue().decode()) +  str(text_string)
        # Method 1: Object.put()
        bucket_name = "ia-mobility-logs"
        object_storage_client = ObjectStorageClient(signer=signer, config={})
        logs_path = "log/mobility/{0}/{1}_part_4_mobility_report_generation_script.txt".format(country,str(input_date))

        object_storage_client.put_object(namespace, bucket_name, logs_path, put_object_body=more_binary_data)
        print("create logs completed")
    except Exception as e:
        print(e)


createLogs()

def writeToLogs(str_arg):
    try:
        f = BytesIO()

        bucket_name = "ia-mobility-logs"
        logs_path = "log/mobility/{0}/{1}_part_4_mobility_report_generation_script.txt".format(country,str(input_date))

        object_storage_client = ObjectStorageClient(signer=signer, config={})
        file_data = object_storage_client.get_object(namespace,bucket_name, logs_path)
        f.seek(0)

        more_binary_data = str(file_data.data.text) +"notebook_name=Mobility Reports;"+  str(str_arg)+"\n"
        object_storage_client.put_object(namespace, bucket_name, logs_path, put_object_body=more_binary_data)
        return "completed"
    except Exception as e:
        print(e)
        return e

def _get_polygon_data(country):
        if country == 'IND':
            return 'oci://ia-customer-insights@bmmp5bv7olp2/required_files/state_city_h3_mapping.csv.gz'
        elif country == 'USA':
            return 'oci://ia-customer-insights@bmmp5bv7olp2/required_files/state_county_h3_mapping.csv.gz'
        elif country == 'ARE':
            return 'oci://ia-customer-insights@bmmp5bv7olp2/required_files/are_polygons.csv'
        else:
            ValueError('Country `{}` data does not exist, existing countries are IND,US,ARE.')

polygon_data_path = _get_polygon_data(country=country)


#baseline_avg_path = "oci://ia-visitation-data@bmmp5bv7olp2/baseline/" +dataset_name + "/"+ baseline_type  + ".csv"
# baseline_avg_path = "s3://ia-customer-insights/baseline/LS/Mar21.csv"

# normalization_path = "s3://ia-customer-insights/contact_tracing/normalized_ratio/"+ dataset_name+ "/*.parquet"
#normalization_path = "s3://ia-customer-insights/contact_tracing/normalized_ratio/"+ dataset_name+ "/ratio_" + input_date + "_with_ratio.parquet"

#normalization_path = "oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/normalized_ratio/"+ dataset_name+"/"+country+"/ratio_" + input_date + "_with_ratio.parquet"

baseline_avg_path = "oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/"+dataset_name+"/delta/baseline/"
normalization_path = "oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/"+ dataset_name+"/delta/normalized_ratio/"

# if(dataset_name == "LS"):
# #     polygon_data_path = "s3://ia-customer-insights/required_files/state_county_h3_mapping.csv.gz,s3://ia-customer-insights/required_files/state_city_h3_mapping_for_MW.csv.gz"
#     polygon_data_path = "s3://ia-customer-insights/required_files/state_city_h3_mapping_for_MW.csv.gz"

# else:    
# #     polygon_data_path = "s3://ia-customer-insights/required_files/state_county_h3_mapping.csv.gz,s3://ia-customer-insights/required_files/state_city_h3_mapping.csv.gz"
#     polygon_data_path = "s3://ia-customer-insights/required_files/state_city_h3_mapping.csv.gz"

#block_path = "s3://ia-customer-insights/contact_tracing/"+ dataset_name +"/mobility_reports/district_block/Updated_" + input_date + "_block_col.parquet"                        ####### 1. MobilityChanges
block_path = "oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/"+ dataset_name +"/delta/mobility_reports/district_block/"
#"oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/"+ dataset_name +"/mobility_reports/district_block/"+country+"/Updated_" + input_date + "_block_col.parquet"

aggregate_path = "oci://ia-visitation-data@bmmp5bv7olp2/contact_tracing/"+ dataset_name +"/delta/mobility_reports/district_aggregate/"
#"oci://ia-customer-insights/contact_tracing/"+ dataset_name +"/mobility_reports/district_aggregate/"+country+"/Updated_" + input_date + "_aggregate_col.parquet"
#aggregate_path = "s3://ia-customer-insights/contact_tracing/"+ dataset_name +"/mobility_reports/district_aggregate/Updated_" + input_date + "_aggregate_col.parquet"            ####### 2. MobilityChanges
    
print(block_path)
print(aggregate_path)

start_time = time.time()
baseline = spark.read.format("delta").load(baseline_avg_path, header=True).where(f"'country' == '{country}' and 'baseline_type' == '{baseline_type}' ")

baseline_da = baseline.filter(baseline.poiType == "District.Aggregate")

baseline_bl = baseline.filter(baseline.poiType == "District.Block")


basleine_da = baseline_da.withColumn("updatedPoiType", lit("admin.district_aggregate"))

basleine_bl = baseline_bl.withColumn("updatedPoiType", lit("admin.district_block"))

baseline_union = basleine_da.union(basleine_bl)

baseline = baseline_union.selectExpr("updatedPoiType as poiType" ,"poiCode","distance_avg","time_avg","moving_avg","static_avg","baselineType","h3resolution","source")

print("baseline")
baseline.show(1)
log_str= "request_id="+requestId+";task_id=1.0;task_name=Read file for baseline;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
normalization = spark.read.format("delta").load(normalization_path, header=True).where(f"'country' == '{country}' and 'normalization_type' == '{normalization_type}' ")
# normalization = normalization.selectExpr("poiCode as poiCode","date as date","user_count as user_count","all_count_avg as all_count_avg ","ratio as ratio","normalization_type as normalizationType")

# normalization = normalization.filter(col("date").between(date_start, date_end))
print("normalization")
normalization.show(1)
log_str= "request_id="+requestId+";task_id=1.1;task_name=Read file for normalization;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
polygon_data = spark.read.format('csv').load(polygon_data_path.split(','), header = 'true')

polygon_data = polygon_data.selectExpr("country as country", "state as state", "city as p_city", "idx9 as idx9")
polygon_data.show(3)
log_str= "request_id="+requestId+";task_id=1.2;task_name=Read file for Polygon mapping;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
aggregate = spark.read.format("delta").load(aggregate_path, header=True).where(f"'country' == '{country}' and date == '{input_date}'")
# from pyspark.sql.functions import lit
# aggregate = aggregate.withColumn('h3_level', lit("9"))
print("aggregator")
aggregate.show(1)
log_str= "request_id="+requestId+";task_id=1.3;task_name=Read file for Aggregate data;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
block = spark.read.format("delta").load(block_path, header=True).where(f"'country' == '{country}' and date == '{input_date}'")
# from pyspark.sql.functions import lit
# block = block.withColumn('h3_level', lit("9"))
print("block")
block.show(1)

log_str= "request_id="+requestId+";task_id=1.4;task_name=Read file for baseline;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

baseline.count(), aggregate.select('date').distinct().count(), block.select('date').distinct().count()

# ud = block.select('date').distinct()
# # ud.orderby('date').show(210)
# ud.sort(col("date").asc()).show(230)

# normalization2 =  normalization.selectExpr("date","city as p_city","ratio as ratio","normalizationType as normalizationType")
# normalization2.show(3)

# mobility_report_block = block.join(baseline, on=["poiCode","poiType"], how='inner')
# mobility_report_block = mobility_report_block.join(polygon_data.select("p_city","idx9"), mobility_report_block.poiCode == polygon_data.idx9, how='inner')

# mobility_report_block.show(3)

# normalization2.show(3)

mobility_report_block = block.join(baseline, on=["poiCode","poiType"], how='inner')
mobility_report_block = mobility_report_block.join(polygon_data.select("p_city","idx9"), mobility_report_block.poiCode == polygon_data.idx9, how='inner')

print("Mobility report block pre join")
mobility_report_block.show(3)

normalization2 =  normalization.selectExpr("date as date","city as city","ratio as ratio","normalizationType as normalizationType")
mobility_report_block = mobility_report_block.join(normalization2, on = ["date","city"], how='inner')

print("mobility_report_block")
mobility_report_block.show(3)

mobility_report_aggregate = aggregate.join(baseline, on=["poiCode","poiType"], how='inner')
mobility_report_aggregate = mobility_report_aggregate.join(normalization.select("date","city","ratio","normalizationType"), on=["date","city"], how='inner')
mobility_report_aggregate.show()

#mobility_report_aggregate.count()

start_time = time.time()
merged = mobility_report_block

merged = merged.withColumn('normalizedDistance', ( merged['distance']/merged['ratio']))
merged = merged.withColumn('normalizedTime', ( merged['time']/merged['ratio']))
merged = merged.withColumn('normalizedStatic', ( merged['static']/merged['ratio']))
merged = merged.withColumn('normalizedMoving', ( merged['moving']/merged['ratio']))


merged = merged.withColumn('changeDistance', ( merged['normalizedDistance'] - merged['distance_avg'] ) / merged['distance_avg'] * 100 + 100 )
merged = merged.withColumn('changeTime', ( merged['normalizedTime'] - merged['time_avg'] ) / merged['time_avg'] * 100 + 100 )
merged = merged.withColumn('changeStatic', ( merged['normalizedStatic'] - merged['static_avg'] ) / merged['static_avg'] * 100 + 100 )
merged = merged.withColumn('changeMoving', ( merged['normalizedMoving'] - merged['moving_avg'] ) / merged['moving_avg'] * 100 + 100 )

merged.columns

merged.columns

# merged = merged.select('date', 'district_code','poiCode', 'poiType', 'normalization_type', 'baseline_type', 'ratio', 'normalized_distance', 'normalized_time', 'normalized_static', 'normalized_moving', 'change_distance', 'change_time', 'change_static', 'change_moving')

# merged = merged.selectExpr('date as date','city as districtCode','poiCode as poiCode', 'poiType as poiType', "normalizationType as normalizationType", "baselineType as baselineType", 'ratio as normalizationFactor', 'normalizedDistance as normalizedDistance', 'normalizedTime as normalizedTime', 'normalizedStatic as normalizedStatic', 'normalizedMoving as normalizedMoving', 'changeDistance as changeDistance', 'changeTime as changeTime', 'changeStatic as changeStatic', 'changeMoving as changeMoving')

merged = merged.selectExpr('date as date','city as districtCode','poiCode as poiCode', 'poiType as poiType', "normalizationType as normalizationType", "baselineType as baselineType", 'distance as distance', 'time as time', 'static as static', 'moving as moving', 'ratio as normalizationFactor', 'distance_avg as distance_avg', 'time_avg as time_avg', 'moving_avg as moving_avg', 'static_avg as static_avg',  'normalizedDistance as normalizedDistance', 'normalizedTime as normalizedTime', 'normalizedStatic as normalizedStatic', 'normalizedMoving as normalizedMoving', 'changeDistance as changeDistance', 'changeTime as changeTime', 'changeStatic as changeStatic', 'changeMoving as changeMoving')

merged = merged.withColumn('h3resolution', lit("9"))

merged.show(3)

merged = merged.withColumn("normalizationFactor", func.round(merged["normalizationFactor"], 2))
merged = merged.withColumn("normalizedDistance", func.round(merged["normalizedDistance"], 2))
merged = merged.withColumn("normalizedTime", func.round(merged["normalizedTime"], 2))
merged = merged.withColumn("normalizedStatic", func.round(merged["normalizedStatic"], 2))
merged = merged.withColumn("normalizedMoving", func.round(merged["normalizedMoving"], 2))
merged = merged.withColumn("changeDistance", func.round(merged["changeDistance"], 2))
merged = merged.withColumn("changeTime", func.round(merged["changeTime"], 2))
merged = merged.withColumn("changeStatic", func.round(merged["changeStatic"], 2))
merged = merged.withColumn("changeMoving", func.round(merged["changeMoving"], 2))

merged.show(3)
log_str= "request_id="+requestId+";task_id=2.1;task_name=Rename and create columns;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

save_string_5 = "oci://ia-customer-insights@bmmp5bv7olp2/intermediate_data/mobility_report_block_" + token +".parquet.gz"
#"s3://ia-customer-insights/intermediate_data/mobility_report_block_" + token +".parquet.gz"
#print(save_string_5)

start_time = time.time()
try:
    merged.write.format("parquet").mode('overwrite').save(save_string_5)
    log_str= "request_id="+requestId+";task_id=2.1;task_name=Write to Intermediat on S3 for block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)
except Exception as e:
    log_str= "request_id="+requestId+";task_id=2.1;task_name=Write to Intermediat on S3 for block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message="+str(e) 
    writeToLogs(log_str)

merged = spark.read.format('parquet').load(save_string_5)
# delta_block_path = "abfss://contact-tracing@iasocialdata.dfs.core.windows.net/contact_tracing/mobility_reports/" + dataset_name + "/" + "normalised_block/" + date_string + "_block_col.csv"
delta_block_path = "s3://ia-customer-insights@/contact_tracing/" + dataset_name + "/delta/mobility_reports/normalised_block/"
# save_string_6 = "abfss://customer-insights@iasocialdata.dfs.core.windows.net/Shubham_data/normalised_block" + date_string +"_block.csv.gz"

#print(delta_block_path)
#merged.show(3)
merged = merged.withColumn('source', lit(dataset_name))
print(delta_block_path)
start_time = time.time()
try:
    merged.write.partitionBy(["country", "date"]).format("delta").mode('append').save(delta_block_path)
    log_str= "request_id="+requestId+";task_id=3.0;task_name=Write to S3 for block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)
except Exception as e:
    log_str= "request_id="+requestId+";task_id=3.0;task_name=Write to S3 for block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message="+str(e) 
    writeToLogs(log_str)

merged.show(3)


start_time = time.time()
#save to database
#try:
#    merged = merged.select('date', 'districtCode', 'poiCode', 'poiType', 'normalizationType', 'baselineType', 'normalizationFactor', 'normalizedDistance', 'normalizedTime', 'normalizedStatic', 'normalizedMoving', 'changeDistance', 'changeTime', 'changeStatic', 'changeMoving')
#    merged_df = merged.select("*").toPandas()
#    renamed_merged_df = merged_df.rename(columns={"changeDistance":"changeRateDistance", "changeTime":"changeRateTime","changeStatic":"changeRateStatic", "changeMoving":"changeRateMoving"}, inplace=False)
#    renamed_merged_df.to_sql('MobilityReportsDBs', con=engine, if_exists='append',index=False,chunksize=5000)
#    log_str= "request_id="+requestId+";task_id=3.1;task_name=Ingest to database for District block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
#         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
#    writeToLogs(log_str)
#except Exception as e:
#    log_str= "request_id="+requestId+";task_id=3.1;task_name=Ingest to database for District block;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
#         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message=" + str(e) 
#    writeToLogs(log_str)

#save to database
# merged_df = merged.select("*").toPandas()
# merged_df.to_sql('MobilityReportsDBs', con=engine, if_exists='append',index=False,chunksize=5000)

start_time = time.time()
merged = mobility_report_aggregate

merged = merged.withColumn('normalizedDistance', ( merged['distance']/merged['ratio']))
merged = merged.withColumn('normalizedTime', ( merged['time']/merged['ratio']))
merged = merged.withColumn('normalizedStatic', ( merged['static']/merged['ratio']))
merged = merged.withColumn('normalizedMoving', ( merged['moving']/merged['ratio']))


merged = merged.withColumn('changeDistance', ( merged['normalizedDistance'] - merged['distance_avg'] ) / merged['distance_avg'] * 100 + 100 )
merged = merged.withColumn('changeTime', ( merged['normalizedTime'] - merged['time_avg'] ) / merged['time_avg'] * 100 + 100 )
merged = merged.withColumn('changeStatic', ( merged['normalizedStatic'] - merged['static_avg'] ) / merged['static_avg'] * 100 + 100 )
merged = merged.withColumn('changeMoving', ( merged['normalizedMoving'] - merged['moving_avg'] ) / merged['moving_avg'] * 100 + 100 )

# merged = merged.select('date','poiCode', 'poiType', "normalizationType", "baselineType", 'ratio', 'normalizedDistance', 'normalizedTime', 'normalizedStatic', 'normalizedMoving', 'changeDistance', 'changeTime', 'changeStatic', 'changeMoving')

merged.columns



# merged = merged.selectExpr('date as date','poiCode as poiCode', 'poiType as poiType', "normalizationType as normalizationType", "baselineType as baselineType", 'ratio as normalizationFactor', 'normalizedDistance as normalizedDistance', 'normalizedTime as normalizedTime', 'normalizedStatic as normalizedStatic', 'normalizedMoving as normalizedMoving', 'changeDistance as changeDistance', 'changeTime as changeTime', 'changeStatic as changeStatic', 'changeMoving as changeMoving')

merged = merged.selectExpr('date as date','poiCode as poiCode', 'poiType as poiType', "normalizationType as normalizationType", "baselineType as baselineType", 'distance as distance', 'time as time', 'static as static', 'moving as moving', 'ratio as normalizationFactor', 'distance_avg as distance_avg', 'time_avg as time_avg', 'moving_avg as moving_avg', 'static_avg as static_avg',  'normalizedDistance as normalizedDistance', 'normalizedTime as normalizedTime', 'normalizedStatic as normalizedStatic', 'normalizedMoving as normalizedMoving', 'changeDistance as changeDistance', 'changeTime as changeTime', 'changeStatic as changeStatic', 'changeMoving as changeMoving')

merged.show(3)

# merged = merged.selectExpr('date as date','poiCode as poiCode', 'poiType as poiType', "normalizationType as normalizationType", "baselineType as baselineType", 'ratio as normalizationFactor', 'normalizedDistance as normalizedDistance', 'normalizedTime as normalizedTime', 'normalizedStatic as normalizedStatic', 'normalizedMoving as normalizedMoving', 'changeDistance as changeDistance', 'changeTime as changeTime', 'changeStatic as changeStatic', 'changeMoving as changeMoving')

merged = merged.withColumn('h3resolution', lit("9"))

#merged.show(3)

merged = merged.withColumn("normalizationFactor", func.round(merged["normalizationFactor"], 2))
merged = merged.withColumn("normalizedDistance", func.round(merged["normalizedDistance"], 2))
merged = merged.withColumn("normalizedTime", func.round(merged["normalizedTime"], 2))
merged = merged.withColumn("normalizedStatic", func.round(merged["normalizedStatic"], 2))
merged = merged.withColumn("normalizedMoving", func.round(merged["normalizedMoving"], 2))
merged = merged.withColumn("changeDistance", func.round(merged["changeDistance"], 2))
merged = merged.withColumn("changeTime", func.round(merged["changeTime"], 2))
merged = merged.withColumn("changeStatic", func.round(merged["changeStatic"], 2))
merged = merged.withColumn("changeMoving", func.round(merged["changeMoving"], 2))
#merged.show(3)
log_str= "request_id="+requestId+";task_id=3.3;task_name=Rename and create column for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

aggregate.select('date').distinct().count(), merged.select('date').distinct().count(), mobility_report_aggregate.select('date').distinct().count()

save_string_3 = "s3://ia-customer-insights/Intermediate_data/mobility_report_aggregate_" + token + dataset_name + ".parquet.gz"
#print(save_string_3)

start_time = time.time()
try:
    merged.write.format("parquet").mode('overwrite').save(save_string_3)
    log_str= "request_id="+requestId+";task_id=3.4;task_name=Write to Intermediate on S3 for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)
except Exception as e:
    log_str= "request_id="+requestId+";task_id=3.4;task_name=Write to Intermediate on S3 for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message="+str(e) 
    writeToLogs(log_str)
    
merged = spark.read.format('parquet').load(save_string_3)
# save_string_4 = "abfss://contact-tracing@iasocialdata.dfs.core.windows.net/contact_tracing/mobility_reports/normalised_aggregate/" + date_string +  token + "_aggregate_col.csv"
# save_string_4 = "abfss://customer-insights@iasocialdata.dfs.core.windows.net/Intermediate_data/normalised_aggregate/" + date_string  + "_aggregate_col.csv"
# delta_district_path = "abfss://contact-tracing@iasocialdata.dfs.core.windows.net/contact_tracing/mobility_reports/" + dataset_name + "/" + "normalised_aggregate/" + date_string + "_aggregate_col.csv"
delta_district_path = "oci://ia-customer-insights@/contact_tracing/" + dataset_name + "/delta/mobility_reports/normalised_aggregate/"
#"s3://ia-customer-insights/contact_tracing/" + dataset_name + "/mobility_reports/normalised_aggregate/"+country+"/"+ input_date + "_aggregate_col.parquet"

#print(delta_district_path)
#merged.show(3)

merged = merged.withColumn('source', lit(dataset_name))
print(delta_district_path)
merged.show()

start_time = time.time()
try:
    merged.write.partitionBy(["country", "date"]).format("delta").mode('append').save(delta_district_path)
    log_str= "request_id="+requestId+";task_id=3.5;task_name=Write to S3 for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)
except Exception as e:
    log_str= "request_id="+requestId+";task_id=3.4;task_name=Write to S3 for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message="+str(e) 
    writeToLogs(log_str)

merged.show()

merged = merged.select('date', 'poiCode', 'poiType', 'normalizationType', 'baselineType', 'normalizationFactor', 'normalizedDistance', 'normalizedTime', 'normalizedStatic', 'normalizedMoving', 'changeDistance', 'changeTime', 'changeStatic', 'changeMoving')
merged = merged.withColumn('districtCode', lit(merged.poiCode))
merged.show(3)

#save to database
#merged = merged.select('date', 'poiCode', 'poiType', 'normalizationType', 'baselineType', 'normalizationFactor', 'normalizedDistance', 'normalizedTime', 'normalizedStatic', 'normalizedMoving', 'changeDistance', 'changeTime', 'changeStatic', 'changeMoving')

start_time = time.time()
#try:
#    merged = merged.select('date', 'districtCode', 'poiCode', 'poiType', 'normalizationType', 'baselineType', 'normalizationFactor', 'normalizedDistance', 'normalizedTime', 'normalizedStatic', 'normalizedMoving', 'changeDistance', 'changeTime', 'changeStatic', 'changeMoving')
#    merged_df = merged.select("*").toPandas()
#    renamed_merged_df = merged_df.rename(columns={"changeDistance":"changeRateDistance", "changeTime":"changeRateTime","changeStatic":"changeRateStatic", "changeMoving":"changeRateMoving"}, inplace=False)

#    renamed_merged_df.to_sql('MobilityReportsDAs', con=engine, if_exists='append',index=False,chunksize=5000)
#    log_str= "request_id="+requestId+";task_id=3.5;task_name=Ingest to the database for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
#         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
#    writeToLogs(log_str)
#except Exception as e:
#    log_str= "request_id="+requestId+";task_id=3.5;task_name=Ingest to the database for district aggregate;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
#         ";task_time_taken="+str(time.time()-start_time)+";status=fail;message=error;error_message="+str(e) 
#    writeToLogs(log_str)

merged.count()

# -------------------------------
cmd_time = time.time()
utc_start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
# -------------------------------  

output_dict = {}
output_dict['delta_block_path'] = delta_block_path
output_dict['delta_district_path'] = delta_district_path

#str_arg += arguments
#

#dbutils.fs.cp("dbfs:" + log_file_name, "abfss://logs@iasocialdata.dfs.core.windows.net/logs",recurse = True)


# Output_path-
#delta_block_path = "s3://ia-customer-insights/contact_tracing/" + dataset_name + "/mobility_reports/normalised_block/" + token + "_block_col.parquet"
#delta_district_path = "s3://ia-customer-insights/contact_tracing/" + dataset_name + "/mobility_reports/normalised_aggregate/" + token + "_aggregate_col.parquet"

arguments = "notebook_name="+ "part_4_mobility_report_generation_script" + ";"+ "Finished the script...." + "utc_start_time="+ utc_start_time + "output_dict="+ str(output_dict)  +"\n"
writeToLogs(arguments)
writeToLogs("delta_block_path="+delta_block_path)
writeToLogs("delta_district_path"+delta_district_path)