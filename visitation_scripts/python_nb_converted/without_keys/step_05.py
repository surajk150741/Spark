print("starting...")



# -*- coding: utf-8 -*-
"""part_1_mobility_script_generic(new_dataset)_new (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGNHBaJAS1w2hsZw4c_Kx4bFdQ0Kzdg_
"""

print("starting...")

from pyspark.sql.window import Window 
import pyspark
from pyspark import SparkContext

from pyspark.sql import functions as F
import pyspark.sql.functions as func


from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.sql.functions import col, window
import pyspark.sql.functions as func
from pyspark.sql.functions import desc
import datetime, time
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.window import Window
import pyspark.sql.functions as sf

import datetime as dt
import numpy as np
import os

from multiprocessing.pool import Pool
import pickle
import pandas as pd

from h3 import h3


from math import radians, cos, sin, asin, sqrt
from pyspark.sql.functions import lit
import json

from datetime import datetime, date, timedelta

#spark.conf.set("fs.azure.account.key.iasocialdata.dfs.core.windows.net",
#"5NXk2QYSRf4tbGSG1Sx20Seg6lkABbERa7ao2h0ZxWbSkLQtuy3+9yYDgr6+FDRKy1Rloi0d2dWa2XmF657SwQ==")
#spark.conf.set("spark.sql.execution.arrow.enabled", "true")  #### Enable Arrow-based columnar data transfers

#sc.install_pypi_package("boto3==1.16.35")
#sc.install_pypi_package("SqlAlchemy")
#sc.install_pypi_package("pymysql")

# from pyspark.sql import SparkSession
# spark = SparkSession.builder.appName("S3CSVRead").getOrCreate()

from pyspark.sql import SparkSession
import sys
import boto3
from io import BytesIO

from functools import reduce
from pyspark.sql import DataFrame

from pyspark.sql.functions import round, col
from dateutil import tz
import sqlalchemy as db


## oci imports
import oci
import os
import io
import sys
from pathlib import Path
from oci.config import validate_config
from oci.object_storage import ObjectStorageClient

#ociconf = oci.config.from_file()

spark = SparkSession.builder.appName("step_05") \
        .config("spark.default.parallelism", '120')\
        .config("spark.scheduler.mode", "FAIR") \
        .config("spark.pyspark.virtualenv.enabled", "true") \
        .config("spark.delta.logStore.oci.impl","io.delta.storage.OracleCloudLogStore")\
        .config("fs.oci.client.custom.authenticator", "com.oracle.bmc.hdfs.auth.InstancePrincipalsCustomAuthenticator")\
        .config('fs.oci.client.hostname', "https://objectstorage.us-ashburn-1.oraclecloud.com")\
        .config('fs.oci.client.auth.tenantId.region', "us-ashburn-1")\
        .getOrCreate()
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")

try:
    spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", AWS_ACCESS_KEY_ID)
    spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
except Exception as e:
    print(e)
    #spark.stop()
#        .config("spark.dynamicAllocation.enabled", "true") \
#        .config("spark.shuffle.service.enabled", "true")\
#        .config("spark.network.timeout","300")\
        
#        .config("spark.cores.max", "190") \
#spark.conf.set('fs.oci.client.auth.tenantId', "ocid1.tenancy.oc1..aaaaaaaamqxmfclvmrazpk4kt7ibkcfzfg2fvg4o2wyzemzu3n7tcpf6nvsa")
#spark.conf.set('fs.oci.client.auth.userId', "ocid1.user.oc1..aaaaaaaa4gjumyz4kiowhzbyjzgajyo5bln565rntgboqod3t6lu4bpwl2ra")
#spark.conf.set('fs.oci.client.auth.fingerprint', "ec:8f:0e:14:e0:31:cd:e4:d6:03:04:aa:aa:94:f9:a9")
#spark.conf.set('fs.oci.client.auth.pemfilepath', "~/.oci/oci_api_key")
#spark.conf.set('fs.oci.client.auth.tenantId.region', "us-ashburn-1")
#spark.conf.set('fs.oci.client.hostname', "https://objectstorage.us-ashburn-1.oraclecloud.com")

print(spark.sparkContext._conf.getAll())
#print(ociconf)
print(spark.sparkContext.defaultParallelism)

client = "all_ind_3.4_all"#dbutils.widgets.get('client')#"Test_silver_level"
input_date= sys.argv[1]
country= sys.argv[2]#"USA"#dbutils.widgets.get('country') #"IND"
indexing_level=12#int(dbutils.widgets.get('indexing_level'))#int("12")
poi_version = '3.4'
source = "GRAVY"

today = (date.today()).strftime("yyyy-mm-dd")
token = f"test_poi_pipeline_{country}_{input_date}_{client}_{source}"



namespace = "bmmp5bv7olp2"
     
signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
identity_client = oci.identity.IdentityClient(config={}, signer=signer)
def create_logs():
        try:
            f = BytesIO()

            bucket_name = "ia-mobility-logs"
            more_binary_data = str(f.getvalue().decode()) + "request_id="+token+";notebook_name=Step 05 processing;start_time="+str(time.time())+"\n request_id="+token+";message=Gold Silver table processing \n"
            #analytics_logs = str(f.getvalue().decode()) +  str(text_string)
            # Method 1: Object.put()

            logs_path = f"logs/{client}/{today}_{input_date}_new_mobility_step_05.txt"
            object_storage_client = ObjectStorageClient(signer=signer, config={})
        
            object_storage_client.put_object(namespace, bucket_name, logs_path, put_object_body=more_binary_data)
        except Exception as e:
            print(e)

create_logs()

def writeToLogs(str_arg):
        try:
            f = BytesIO()
            bucket_name = "ia-mobility-logs"
            logs_path = f"logs/{client}/{today}_{input_date}_new_mobility_step_05.txt"
            object_storage_client = ObjectStorageClient(signer=signer, config={})
            file_data = object_storage_client.get_object(namespace,bucket_name, logs_path)
        
            more_binary_data = str(file_data.data.text)+"\n"+"notebook_name=Step 05 processing;" + str(str_arg)
            #analytics_logs = str(f.getvalue().decode()) +  str(text_string)
            # Method 1: Object.put()
            object_storage_client.put_object(namespace, bucket_name, logs_path, put_object_body=more_binary_data)
            print(str_arg)
            return "completed"
        
        except Exception as e:
            print(e)
            return e

start_time = time.time()
source_path = f"oci://ia-visitation-data@bmmp5bv7olp2/poiVisit/delta/{country}/all_{country.lower()}_all_poi_v{poi_version}/"+str(indexing_level)+"/"
#f"oci://ia-customer-insights@bmmp5bv7olp2/poiVisit/delta/IND/" +"all_ind_all_poi_incremented/12/"

df_new = spark.read.format("delta").load(source_path, headers=True).where(col("date") == input_date)
#df_new.show(3)

df_new = df_new.withColumn("country", lit(str(country)))\
                .withColumn("year", col("year").cast("integer"))



log_str= "request_id="+token+";task_id=1.1;task_name=Read file from step02;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 

writeToLogs(log_str)
save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/ifas/all_poi_{poi_version.replace('.','_')}/12"

s3_save_string = f"s3a://ia-visitation-data/step_05/ifas/all_poi_{poi_version.replace('.','_')}/12"

try:
    df = spark.read.format("delta").load(save_string, headers=True).where(f"country = '{country}' and date = '{input_date}'")
    if(df.count() == 0):
         raise Exception("No rows found")
    print("Existing data found so write skipped")     
except Exception as e:
    print("exception")
    print(e)
    print(f"Read failed no data found for all_{country.lower()}_all_poi in {country} and {input_date}")
    start_time = time.time()
    df_new.write.partitionBy(["country","source","year","month","date"]).format("delta")\
        .mode('append').option("timestampFormat", "yyyy-MM-dd HH:mm:ss").save(save_string,header =True)

    log_str= "request_id="+token+";task_id=1.2.1;task_name=Write to oci;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
             ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)

    #save to s3
    start_time = time.time()
    df_new.write.partitionBy(["country","source","year","month","date"]).format("delta")\
        .mode('append').option("timestampFormat", "yyyy-MM-dd HH:mm:ss").save(s3_save_string,header =True)

    log_str= "request_id="+token+";task_id=1.2.2;task_name=Write to s3;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)
    #.option("overwriteschema", "true")\

start_time = time.time()
df = spark.read.format("delta").load(save_string, headers=True).where(col("date") == input_date)

#df.show(3)

log_str= "request_id="+token+";task_id=1.3.1;task_name=read from oci;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

#df.groupBy("version").count().show(10)

#spark.stop()
poi_path = "oci://ia-customer-insights@bmmp5bv7olp2/poi_data/delta/updated/POIs_v3.4_jan"
#f"oci://ia-customer-insights@bmmp5bv7olp2/poi_data/delta/updated/POIs_v3.1.6_new"#poi_data/delta/updated/POIs_v3.1.2/"

start_time = time.time()
poi_df = spark.read.format('delta').load(poi_path, header=True).filter(col("country") == country)

#poi_df.show(3)

log_str= "request_id="+token+";task_id=1.3.2;task_name=Read poi data;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
df_visitation = df.withColumnRenamed("poiTypeId", "poiType")\
                .withColumnRenamed("source", "vendorSource")\
                .withColumnRenamed("version", "visit_version")\


step_5_df = df_visitation.join(poi_df, how="inner", on=["poi_type", "poi_code", "country", "district_code"])

step_5_df.dropDuplicates()
step_5_df = step_5_df.withColumnRenamed("poi_type", "poiType")\
                     .withColumnRenamed("poi_code", "poiCode")



log_str= "request_id="+token+";task_id=1.3.3;task_name=Join poi and step02;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

#step_5_df.show(3)

start_time = time.time()
save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/all_join_data_{poi_version.replace('.','_')}/"
try:
    step_5_df_ = spark.read.format("parquet").load(save_string, header=True).where(f"country = '{country}' and date = '{input_date}'")
    print(f"Read failed data found for all_join_data in {country} and {input_date}")
    if step_5_df_.count() == 0:
         raise Exception("No rows found")
except Exception as e:
    print(e)
    print(f"Read failed no data found for all_join_data in {country} and {input_date}")
    step_5_df.write.partitionBy(["country", "vendorSource","year","date"])\
        .mode('append')\
        .format('parquet')\
        .save(save_string, header=True)
    log_str= "request_id="+token+";task_id=1.3.4;task_name=Save all join temp data on oci;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
    writeToLogs(log_str)


step_5_df_visit = step_5_df.groupBy(col("country"),col("vendorSource"),col("poiType"), col("date"), col("month"), col("year"),col("customer"), col("h3index"), col("district_code")).count()
print("h9s")
step_5_df_visit.show(3)


start_time = time.time()
save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/h9s/all_visitation_data_{poi_version.replace('.','_')}/"
step_5_df_visit.write.partitionBy(["country","customer","vendorSource","year","date"])\
.mode('append')\
.format('delta')\
.save(save_string, header=True)

#.option("overwriteSchema", "true")\

log_str= "request_id="+token+";task_id=1.4.1;task_name=Save all visitation on h9 on oci;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

start_time = time.time()
# Write to s3
s3_save_string = f"s3a://ia-visitation-data/step_05/h9s/all_visitation_data_{poi_version.replace('.','_')}/"
step_5_df_visit.write.partitionBy(["country","customer","vendorSource","year","date"])\
.mode('append')\
.format('delta')\
.save(s3_save_string, header=True)
#.option("overwriteSchema", "true")\

log_str= "request_id="+token+";task_id=1.4.2;task_name=Save all visitation on h9 on s3;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

#save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/all_visitation_data/"
#step_5_df_visit = spark.read.format("delta").load(save_string, header=True).where(col("country") == country and col("date") == input_date)

start_time = time.time()
save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/all_join_data_{poi_version.replace('.','_')}/"
#where_clause = f"country = '{str(country)}' & date = '{str(input_date)}'"
step_5_df = spark.read.format("parquet").load(save_string, header=True).where(f"country = '{country}' and date = '{input_date}'")

#step_5_df = step_5_df.filter(col("country") == str(country))

log_str= "request_id="+token+";task_id=1.4.3;task_name=Rad all join datatask_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

#step_5_df.show(3)
step_5_df_visit = step_5_df.groupBy(col("country"),col("version"),col("vendorSource"),col("poiType"),col("poiCode"), col("name"),col("date"), col("month"),col("year"), 
                                    col("customer"), col("h3index"), col("district_code"), col("district")).count()

print("pois")
step_5_df_visit.show(3)
start_time = time.time()
save_string = f"oci://ia-visitation-data@bmmp5bv7olp2/step_05/pois/all_poi_visitation_data_{poi_version.replace('.','_')}/"
step_5_df_visit.write.partitionBy(["country","version","customer","vendorSource","year","date"])\
.mode('append')\
.format('delta')\
.option("overwriteSchema", "true")\
.save(save_string, header=True)

log_str= "request_id="+token+";task_id=1.4.4;task_name=Save all poi visitation data on oci;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)

#spark._jsc.hadoopConfiguration().set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')
start_time = time.time()
s3_save_string = f"s3a://ia-visitation-data/step_05/pois/all_poi_visitation_data_{poi_version.replace('.','_')}/"
step_5_df_visit.write.partitionBy(["country","customer","vendorSource","year","date"]).mode('append')\
.format('delta').option("overwriteSchema", "true").save(s3_save_string, header=True)

log_str= "request_id="+token+";task_id=1.4.5;task_name=Save all Poi visiation data on s3;task_start_time="+str(start_time)+";task_end_time="+str(time.time())+\
         ";task_time_taken="+str(time.time()-start_time)+";status=suceessful;message=ok" 
writeToLogs(log_str)
spark.stop()
