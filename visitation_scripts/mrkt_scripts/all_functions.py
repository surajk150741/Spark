print("starting...")



# -*- coding: utf-8 -*-
"""part_1_mobility_script_generic(new_dataset)_new (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGNHBaJAS1w2hsZw4c_Kx4bFdQ0Kzdg_
"""

print("starting...")

from pyspark.sql.window import Window 
import pyspark
from pyspark import SparkContext

from pyspark.sql import functions as F
import pyspark.sql.functions as func


from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.sql.functions import col, window
import pyspark.sql.functions as func
from pyspark.sql.functions import desc
import datetime, time

from pyspark.sql.types import IntegerType
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.window import Window
import pyspark.sql.functions as sf

import datetime as dt
import numpy as np
import os

from multiprocessing.pool import Pool
import pickle
import pandas as pd

from h3 import h3


from math import radians, cos, sin, asin, sqrt
from pyspark.sql.functions import lit
import json

from datetime import datetime, date, timedelta

#spark.conf.set("fs.azure.account.key.iasocialdata.dfs.core.windows.net",
#"5NXk2QYSRf4tbGSG1Sx20Seg6lkABbERa7ao2h0ZxWbSkLQtuy3+9yYDgr6+FDRKy1Rloi0d2dWa2XmF657SwQ==")
#spark.conf.set("spark.sql.execution.arrow.enabled", "true")  #### Enable Arrow-based columnar data transfers

#sc.install_pypi_package("boto3==1.16.35")
#sc.install_pypi_package("SqlAlchemy")
#sc.install_pypi_package("pymysql")

# from pyspark.sql import SparkSession
# spark = SparkSession.builder.appName("S3CSVRead").getOrCreate()

#
#accessKeyId =  dbutils.secrets.get(scope = "old-aws-account", key = "privateKey")
#secretAccessKey = dbutils.secrets.get(scope = "old-aws-account", key = "secretAccessKey")

from pyspark.sql import SparkSession
import sys
import boto3
from io import BytesIO

from functools import reduce
from pyspark.sql import DataFrame

from pyspark.sql.functions import round, col
from dateutil import tz
import sqlalchemy as db


import h3_pyspark
#spark.sql.autoBroadcastJoinThreshold = -1

## oci imports
import oci
import os
import io
import sys
from pathlib import Path
from oci.config import validate_config
from oci.object_storage import ObjectStorageClient

#ociconf = oci.config.from_file()

spark = SparkSession.builder.appName("step4analysis") \
        .config("spark.driver.memory", "40g") \
        .config("spark.driver.cores", "8") \
        .config("spark.executor.memory", "9g") \
        .config("spark.executor.cores", "5") \
        .config("spark.scheduler.mode", "FAIR") \
        .config("spark.executor.instances", '23')\
        .config("spark.default.parallelism", '228')\
        .config("spark.delta.logStore.oci.impl","io.delta.storage.OracleCloudLogStore")\
        .config("fs.oci.client.custom.authenticator", "com.oracle.bmc.hdfs.auth.InstancePrincipalsCustomAuthenticator")\
        .getOrCreate()
#        .config("spark.dynamicAllocation.enabled", "true") \
#        .config("spark.shuffle.service.enabled", "true")\
#        .config("spark.network.timeout","300")\
        
#        .config("spark.cores.max", "190") \
#spark.conf.set('fs.oci.client.auth.tenantId', "ocid1.tenancy.oc1..aaaaaaaamqxmfclvmrazpk4kt7ibkcfzfg2fvg4o2wyzemzu3n7tcpf6nvsa")
#spark.conf.set('fs.oci.client.auth.userId', "ocid1.user.oc1..aaaaaaaa4gjumyz4kiowhzbyjzgajyo5bln565rntgboqod3t6lu4bpwl2ra")
#spark.conf.set('fs.oci.client.auth.fingerprint', "ec:8f:0e:14:e0:31:cd:e4:d6:03:04:aa:aa:94:f9:a9")
#spark.conf.set('fs.oci.client.auth.pemfilepath', "~/.oci/oci_api_key")
#spark.conf.set('fs.oci.client.auth.tenantId.region', "us-ashburn-1")
#spark.conf.set('fs.oci.client.hostname', "https://objectstorage.us-ashburn-1.oraclecloud.com")

print(spark.sparkContext._conf.getAll())
#print(ociconf)
print(spark.sparkContext.defaultParallelism)
 
from parameters import *

class inputs:
     def __init__(self):
      
        saved_path = f"oci://ia-customer-insights@bmmp5bv7olp2/poi_data/delta/updated/POIs_v3.4_feb"
        self.poi = spark.read.format('delta').load(saved_path, header=True)
        save = f"oci://ia-visitation-data@bmmp5bv7olp2/poiVisit/delta/IND/all_ind_all_poi_v3.4/12/"
        self.visits = spark.read.format('delta').load(save, header=True)


inputs = inputs()
poi = inputs.poi
visits = inputs.visits

################### poi_filters ####################

def brand_data(poitype_brands):
    dfb = poi.filter(col('poi_type').isin(poitype_brands))
    l2 = dfb.select('brands').rdd.flatMap(lambda x: x).collect()
    l4 = filterNoneType(l2)
    saved_path = f"oci://ia-customer-insights@bmmp5bv7olp2/poi_data/delta/updated/POIs_v3.4_feb"
    df_poi_b = spark.read.format('delta').load(saved_path, header=True).where((col('brands').isin(l4)) & (col('poi_type').isin(poitype_brands)))
    return df_poi_b  

def source_data(source):
    dfs = poi.filter(col('source_client_name').isin(source))
    return dfs

def google_data(category):
    dfg = poi.filter(col('google_category').isin(category))
    return dfg

def name_data(names):
    dfn = poi.filter(col('name').isin(names))
    return dfn


############# rest #######


def num_repartition(x):
    if x <= 1000000:
        return 1
    elif x <= 10000000:
        y = int(str(x)[:1])
        return y+1
    else:
        y = int(str(x)[:2])
        return y+1

def write_data(df_final, customer, customer_sub, city_name, token, x):
    saved_path = f"oci://ia-marketing-data@bmmp5bv7olp2/customers/{customer}/{customer_sub}/{city_name}/{token}/"
    df_final.repartition(num_repartition(x)).write.format('csv').mode('overwrite').save(saved_path,header=True)
    return None  ### HOW TO RETURN NOTHING IN A FUNCTION? 

def filterNoneType(lis):
    lis2 = []
    for l in lis: #filter out NoneType
        if type(l) == str:
            lis2.append(l)
    return lis2

def poitype_data(df, poitype):
    dfp = df.filter(col('poi_type').isin(poitype))
    return dfp   

def wbi_data(client, case, token):
    saved_path = f"oci://ia-visitation-data@bmmp5bv7olp2/dump/{client}/wbi/{case}/{token}"
    dfw = spark.read.format('csv').load(saved_path, header=True)
    return dfw

def district_code_data(df, cities):
    dfc = df.filter(col('district_code').isin(cities))
    return dfc

def date_data(df, dates):
    dfd = df.filter(col('date').isin(dates))
    return dfd

def count_data(df):
    dfs = df.select('ifa')
    dfc = dfs.dropDuplicates()
    x = dfc.count()
    return dfc, x

def join_data(df1,df2):
    dfj = df1.join(df2, on=['poi_type','poi_code'],how='inner').select(df1.ifa)
    #dffff, x = count_data(dfj)
    return dfj 

def date_list(date1, date2):
    start_date = date1
    end_date = date2
    #start_date_value = datetime.datetime.strptime(start_date, "%Y-%m-%d").date()
    #start_date_day = (datetime.datetime.now().date() - start_date_value).days
    start_date_value = dt.datetime.strptime(start_date, "%Y-%m-%d").date()
    start_date_day = (dt.datetime.now().date() - start_date_value).days
    
    #get no of days from today till end date.
    #end_date_value = datetime.datetime.strptime(end_date, "%Y-%m-%d").date()
    #end_date_day = (datetime.datetime.now().date() - end_date_value).days
    end_date_value = dt.datetime.strptime(end_date, "%Y-%m-%d").date()
    end_date_day = (dt.datetime.now().date() - end_date_value).days

    #create a list of number of days from start till end 
    number_of_days = [x for x in range(end_date_day,start_date_day+1)]
    number_of_days.sort(reverse=True)

    #day_values =[ (datetime.datetime.now()-datetime.timedelta(days = x)).strftime("%Y-%m-%d") for x in number_of_days]
    day_values =[ (dt.datetime.now()-dt.timedelta(days = x)).strftime("%Y-%m-%d") for x in number_of_days]
    day_values
    date_list = day_values
    return date_list

def poi_district_visits(poitype_list, district_list, date_1, date_2):
    d_list = date_list(date_1, date_2)
    df_date = date_data(visits, d_list)
    df_city = district_code_data(df_date, district_list)
    df_final = poitype_data(df_city, poitype_list)
    return df_final


